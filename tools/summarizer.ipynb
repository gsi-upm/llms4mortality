{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    While we can control the context size for most ollama models tampering with num_ctx, this is not\\n    the full story. num_ctx includes the size of the answer given by the model, so\\n    when dealing with complex and long prompts this might lead to incomplete, short or incoherent answers.\\n    We opted for truncating long reports in favor of avoiding this issues. max_chars parameter controls this truncation\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: (On report length and limitations of LLMs)\n",
    "\"\"\"\n",
    "    While we can control the context size for most ollama models tampering with num_ctx, this is not\n",
    "    the full story. num_ctx includes the size of the answer given by the model, so\n",
    "    when dealing with complex and long prompts this might lead to incomplete, short or incoherent answers.\n",
    "    We opted for truncating long reports in favor of avoiding this issues. max_chars parameter controls this truncation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Project path is /home/daucco/ownCloud-UPM/CBR/llms4mortality\n"
     ]
    }
   ],
   "source": [
    "## Project root path\n",
    "pjpath = ''\n",
    "\n",
    "# Hacky way of finding the project's root path. Do not rely on this, set your own pjpath!\n",
    "for p in Path.cwd().parents:\n",
    "    if p.stem == 'llms4mortality':\n",
    "        pjpath = p\n",
    "        break\n",
    "\n",
    "print(f'> Project path is {pjpath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant paths\n",
    "mimicpath = pjpath / 'data/mimiciv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Controls which data to load\n",
    "samp_size = 5000\n",
    "balanced_data = True\n",
    "target_split = False  # Split set from loaded dataframe to generate summaries from. If None, we disregard inner splits and take entries directly from the main loaded dataframe\n",
    "\n",
    "base_models = [\n",
    "    'll3',\n",
    "    #'medgenius32b',\n",
    "    #'dsmedical8b',\n",
    "    #'biomistral7b'\n",
    "]\n",
    "\n",
    "# Slice long input: Just keep up to max_words of each text\n",
    "max_chars = 22000\n",
    "subsamp_size = False  # 200, 100 Number of entries to test model with. Or False to disregard it\n",
    "\n",
    "# This is the collection of columns that contains the relevant patient info\n",
    "#   Values remap column name to an alternative and more readable name (might be useful if using LLMs)\n",
    "pdc_remap = {\n",
    "    'age': 'AGE',\n",
    "    'gender': 'GENDER',\n",
    "    'marital_status': 'MARITAL STATUS',\n",
    "    'race': 'RACE',\n",
    "    'diagnose_group_description': 'BROAD DIAGNOSIS',\n",
    "    'diagnose_group_mortality': 'MORTALITY RISK',\n",
    "    'insurance': 'INSURANCE',\n",
    "    #'text': 'REPORT'\n",
    "}\n",
    "\n",
    "prepend_extended_patient_data = True    # If set True, preprends additional categorical data from pdf_remap to the beginning of the text entry for each patient\n",
    "\n",
    "# Ollama hyperparams\n",
    "n_ctx = 32   # Context length (x 1024)\n",
    "temp = 0.0 # Temperature option for the LLM. The greater, the more creative the answer (def 0.1)\n",
    "top_k = 20\n",
    "top_p = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load precomputed dataframe.\n",
    "df = pd.read_csv(mimicpath / f'mimiciv_4_mortality_S{samp_size}{'_balanced' if balanced_data else ''}.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> (!) Preprending additional patient data to text!\n"
     ]
    }
   ],
   "source": [
    "# Load precomputed dataframe.\n",
    "df = pd.read_csv(mimicpath / f'mimiciv_4_mortality_S{samp_size}{'_balanced' if balanced_data else ''}.csv.gz')\n",
    "\n",
    "if target_split:\n",
    "    print(f'>> (i) Performing summarization only on entries from {target_split} split...')\n",
    "    # Load precomputed splits\n",
    "    with open(mimicpath / f'hadmid_splits_S{samp_size}{'_balanced' if balanced_data else ''}.json', 'r') as ifile:\n",
    "        splits_hadmids = json.load(ifile)\n",
    "\n",
    "    # Loads target split\n",
    "    df = df.set_index('hadm_id').loc[splits_hadmids['test']]\n",
    "\n",
    "# Do further subsamplig (do this just to speed up computations)\n",
    "if subsamp_size:\n",
    "    print(f'>> (!) Subsampling total entries to {subsamp_size}!')\n",
    "    print(f'> Subsampling test data to {subsamp_size}...')\n",
    "    df = df.sample(subsamp_size, random_state=SEED)\n",
    "\n",
    "# Preprends additional data to the text entries\n",
    "if prepend_extended_patient_data:\n",
    "    print(f'>> (!) Preprending additional patient data to text!')\n",
    "    df['text'] = df.apply(lambda x: ''.join([f'{p_cremap}: {str(x[p_cname]).replace('_', ' ')}\\n' for p_cname, p_cremap in pdc_remap.items()]) + x['text'], axis=1)\n",
    "\n",
    "# We are only interested in texts (and hadm_id ad index)\n",
    "df = df.set_index('hadm_id')['text'].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hadm_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21891113</th>\n",
       "      <td>AGE: 68\\nGENDER: F\\nMARITAL STATUS: DIVORCED\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29643114</th>\n",
       "      <td>AGE: 96\\nGENDER: M\\nMARITAL STATUS: WIDOWED\\nR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26747385</th>\n",
       "      <td>AGE: 47\\nGENDER: F\\nMARITAL STATUS: MARRIED\\nR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23932127</th>\n",
       "      <td>AGE: 64\\nGENDER: M\\nMARITAL STATUS: MARRIED\\nR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27210508</th>\n",
       "      <td>AGE: 72\\nGENDER: M\\nMARITAL STATUS: MARRIED\\nR...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       text\n",
       "hadm_id                                                    \n",
       "21891113  AGE: 68\\nGENDER: F\\nMARITAL STATUS: DIVORCED\\n...\n",
       "29643114  AGE: 96\\nGENDER: M\\nMARITAL STATUS: WIDOWED\\nR...\n",
       "26747385  AGE: 47\\nGENDER: F\\nMARITAL STATUS: MARRIED\\nR...\n",
       "23932127  AGE: 64\\nGENDER: M\\nMARITAL STATUS: MARRIED\\nR...\n",
       "27210508  AGE: 72\\nGENDER: M\\nMARITAL STATUS: MARRIED\\nR..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = 'http://localhost:11434/api/generate'\n",
    "auth_cookie = ''\n",
    "\n",
    "for base_model in base_models:\n",
    "    print(f'> [BASE MODEL]: {base_model}')\n",
    "\n",
    "    model = f'{base_model}_summarizer'\n",
    "\n",
    "    responses = {}\n",
    "    i=1\n",
    "\n",
    "    # Resolves responses disk path\n",
    "    summary_id = f'summary_S{str(samp_size)}{'_balanced' if balanced_data else ''}{'_sp' + target_split if target_split else ''}_{base_model}_mc{str(max_chars)}{'_ss' + str(subsamp_size) if subsamp_size else ''}'\n",
    "    summary_path = Path(f'{mimicpath}/{summary_id}.csv')\n",
    "\n",
    "    if summary_path.is_file():\n",
    "        # Loads existing file and assumes it contains the same data structure as the generated output (ie, is a compatible dataframe)\n",
    "        print(f'>> (i) Target file already exists. Parsing contents and updating entries to process...')\n",
    "        \n",
    "        _df_existing_responses = pd.read_csv(summary_path, index_col=0)\n",
    "        assert list(_df_existing_responses.columns) == ['SUMMARY']\n",
    "\n",
    "        precomputed_indices = _df_existing_responses.index\n",
    "        print(f'>> (i) {len(precomputed_indices)} indices were found in precomputed results file and will be ommitted from current execution...')\n",
    "        df = df.loc[list(set(df.index) - set(precomputed_indices))]\n",
    "\n",
    "\n",
    "    else:\n",
    "        # Initializes empty df where summaries will be saved online:\n",
    "        pd.DataFrame(columns=['SUMMARY']).to_csv(summary_path, mode='w', header=True)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        print(f'>> Processing row {i} out of {len(df)}', end='\\r')\n",
    "\n",
    "        # Get text from entry\n",
    "        text = row['text']\n",
    "\n",
    "        # Truncate middle if resulting text is longer than max_chars\n",
    "        if len(text) > max_chars:\n",
    "            print(f'>> (!) Text exceeds the max char limit ({len(text)}) in entry {index}. Middle-truncating to {max_chars}...')\n",
    "            text = text[:(max_chars//2)] + text[-(max_chars//2):]\n",
    "            print(f'\\t... Result truncate: {len(text)}')\n",
    "\n",
    "        formatted_input = json.dumps({'REPORT': text})\n",
    "        \n",
    "        data = {'model': model,  # Explicit model to use\n",
    "                'options': {\n",
    "                                'num_ctx': n_ctx * 1024,\n",
    "                                'temperature': temp, # 0?\n",
    "                                'seed': SEED,\n",
    "                                'top_k': top_k,\n",
    "                                'top_p': top_p\n",
    "                                },\n",
    "                'keep-alive': -1,  # Keep connection open\n",
    "                'prompt': formatted_input,\n",
    "                'stream': False,  # Wait and return all the result at once\n",
    "                'format': {  # Prognosis and mortality      \n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                    'SUMMARY': {\n",
    "                        'type': 'string'\n",
    "                    }\n",
    "                },\n",
    "                'required': [\n",
    "                    'SUMMARY'\n",
    "                ]\n",
    "                }\n",
    "            }\n",
    "        # Prepares query\n",
    "        data = json.dumps(data)\n",
    "        cookies = {\n",
    "            '_oauth2_proxy': auth_cookie}\n",
    "        headers = {\n",
    "            'Content-Type': 'application/x-www-form-urlencoded',\n",
    "        }\n",
    "\n",
    "        response = requests.post(instance, cookies=cookies, headers=headers, data=data)\n",
    "        response = json.loads(response.text)['response']\n",
    "        #responses[index] = json.loads(response) # Keeps the dictionary version of the json response\n",
    "\n",
    "        # Save online\n",
    "        df_response = pd.Series({index: json.loads(response)['SUMMARY']}).to_frame()\n",
    "        df_response.to_csv(summary_path, mode='a', header=False)\n",
    "        i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
