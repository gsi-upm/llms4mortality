{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a helper notebook to find and prepare the neighbours of each entry of the test set\n",
    "# Ideally the neighbours should be resolved online, but having a precomputed set of neighbours\n",
    "#   related to each entry of the test set speeds up experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from gsicbr.mincbr import CBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Project path is /home/daucco/ownCloud-UPM/CBR/medical-cbr\n"
     ]
    }
   ],
   "source": [
    "## Project root path\n",
    "pjpath = ''\n",
    "\n",
    "# Hacky way of finding the project's root path. Do not rely on this, set your own pjpath!\n",
    "for p in Path.cwd().parents:\n",
    "    if p.stem == 'medical-cbr':\n",
    "        pjpath = p\n",
    "        break\n",
    "\n",
    "print(f'> Project path is {pjpath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant paths\n",
    "mimicpath = pjpath / 'datasets/mimiciv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Controls which data to load\n",
    "samp_size = 20000\n",
    "balanced_data = True\n",
    "\n",
    "# Embeddings (precomputed best for k=1)\n",
    "emb_modname, emb_mod_truncation = ('alldistilrobertav1', 'middle')\n",
    "\n",
    "summarizer_model = 'iv_ll3_summarizer'  # Needs to be available\n",
    "\n",
    "# Slice long input: Just keep up to max_words of each text\n",
    "max_chars = 25000\n",
    "subsamp_size = 1000  # 100 Number of entries to test model with\n",
    "\n",
    "# LLM parameters\n",
    "# NOTE TODO: Fixed if just one, if using multiple, deifne a list of dictionaries each one containing the specific info for each model, the run in batch to generate summaries\n",
    "input_type = 'S'    # (R)eport, (R)eport and (C)hart data as json\n",
    "\n",
    "# This is the collection of columns that contains the relevant patient info that will be provided to the LLM with the text report\n",
    "# Remapping some column names to make them more significant in the prompt\n",
    "pdc_remap = {\n",
    "    'age': 'AGE',\n",
    "    'gender': 'GENDER',\n",
    "    'marital_status': 'MARITAL STATUS',\n",
    "    'race': 'RACE',\n",
    "    'diagnose_group_description': 'BROAD DIAGNOSIS',\n",
    "    'diagnose_group_mortality': 'MORTALITY RISK',\n",
    "    'insurance': 'INSURANCE',\n",
    "    #'text': 'REPORT'\n",
    "}\n",
    "\n",
    "n_ctx = 32   # Context length (x 1024)\n",
    "m_span = 'amonth' # How to ask the LLM for mortality (count days vs month)\n",
    "temp = 0.1 # Temperature option for the LLM. The greater, the more creative the answer (def 0.1)\n",
    "top_k = 20\n",
    "top_p = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load precomputed dataframe. Keeps only hadm_id and delta_days_dod (to find patients that died after n days discharge)\n",
    "# Transform to boolean (patient died within 30 days after discharge)\n",
    "#df = pd.read_csv(mimicpath / f'mimiciv_4_mortality_S{samp_size}.csv.gz')[['hadm_id', 'delta_days_dod']]\n",
    "df = pd.read_csv(mimicpath / f'mimiciv_4_mortality_S{samp_size}{'_balanced' if balanced_data else ''}.csv.gz')\n",
    "df['DIES'] = df['delta_days_dod'].apply(lambda x: x > 0 and x <= 30)\n",
    "\n",
    "# Load precomputed splits\n",
    "with open(mimicpath / f'hadmid_splits_S{samp_size}{'_balanced' if balanced_data else ''}.json', 'r') as ifile:\n",
    "    splits_hadmids = json.load(ifile)\n",
    "\n",
    "# Load sorted hadm_ids from disk. This is required to be able to locate the appropriate embeddings for each entry\n",
    "with open(mimicpath / f'hadmid_sorted_S{samp_size}{'_balanced' if balanced_data else ''}.json', 'r') as ifile:\n",
    "    emb_hadmids = json.load(ifile)['HADM_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading embeddings from embeddings_alldistilrobertav1_S20000_Tmiddle_balanced.npy...\n",
      "> Subsampling test data to 100...\n"
     ]
    }
   ],
   "source": [
    "# Embedding load\n",
    "emb_modname = re.sub('[^a-zA-Z0-9]+', '', emb_modname)\n",
    "emb_mod_fname = f'embeddings_{emb_modname}_S{samp_size}_T{emb_mod_truncation}{'_balanced' if balanced_data else ''}.npy'\n",
    "print(f'> Loading embeddings from {emb_mod_fname}...')\n",
    "embeddings = np.load(mimicpath / emb_mod_fname)\n",
    "\n",
    "# Locates data (case definitions (X) + solutions (y))\n",
    "hadm2idx = {hadm: i for i, hadm in enumerate(emb_hadmids)}\n",
    "\n",
    "# Entries to be used as CB\n",
    "X_cb = embeddings[[hadm2idx[hadm] for hadm in splits_hadmids['cb']]]\n",
    "df_cb = df.set_index('hadm_id').loc[splits_hadmids['cb']]\n",
    "y_cb = df_cb['DIES'].values[:, np.newaxis]\n",
    "\n",
    "# Train data for CBR model\n",
    "X_train = embeddings[[hadm2idx[hadm] for hadm in splits_hadmids['train']]]\n",
    "df_train = df.set_index('hadm_id').loc[splits_hadmids['train']]\n",
    "y_train = df_train['DIES'].values[:, np.newaxis]\n",
    "\n",
    "# Entries to be used as test\n",
    "df_test = df.set_index('hadm_id').loc[splits_hadmids['test']]\n",
    "# Gets test subsample\n",
    "if subsamp_size:\n",
    "    print(f'> Subsampling test data to {subsamp_size}...')\n",
    "    df_test = df_test.sample(subsamp_size, random_state=SEED)\n",
    "\n",
    "y_test = df_test['DIES'].values[:, np.newaxis]\n",
    "X_test = embeddings[[hadm2idx[hadm] for hadm in df_test.index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CBR module\n",
    "# Fixed params\n",
    "agg_strat = 'vot'\n",
    "# Recovery params\n",
    "rec_kwargs = {\n",
    "    'retrieve_algo': 'brute',\n",
    "    'retrieve_metric': 'cosine'\n",
    "}\n",
    "cbr = CBR(X_cb, y_cb, agg_strat=agg_strat, rec_kwargs=rec_kwargs, seed=SEED)\n",
    "\n",
    "# Fits CBR components\n",
    "cbr.fit(X_train, y_train, k=1, weighted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(i) Text exceeds the max char limit (37342) in entry Index(['note_id', 'subject_id', 'charttime', 'text', 'gender', 'dod',\n",
      "       'anchor_age', 'anchor_year', 'admittime', 'admission_type', 'insurance',\n",
      "       'marital_status', 'race', 'diagnose_group_description', 'drg_mortality',\n",
      "       'diagnose_group_mortality', 'age', 'delta_days_dod', 'DIES'],\n",
      "      dtype='object'). Middle-truncating to 25000...\n",
      "... Result truncate: 25000\n",
      ">> Processing row 100 out of 100\r"
     ]
    }
   ],
   "source": [
    "instance = 'http://localhost:11434/api/generate'\n",
    "auth_cookie = ''\n",
    "\n",
    "model = summarizer_model\n",
    "\n",
    "responses = {}\n",
    "ii=1\n",
    "\n",
    "# Prepends additional patient data to text report before feeding summarizer\n",
    "df_test['text'] = df_test.apply(lambda x: ''.join([f'{p_cremap}: {str(x[p_cname]).replace('_', ' ')}\\n' for p_cname, p_cremap in pdc_remap.items()]) + x['text'], axis=1)\n",
    "\n",
    "for i, itrow in enumerate(df_test.iterrows()):\n",
    "    print(f'>> Processing row {ii} out of {len(df_test)}', end='\\r')\n",
    "\n",
    "    # Get text from entry\n",
    "    #text = row['text']\n",
    "\n",
    "    # Gets neigbhour from cb using index\n",
    "    hadm_idx, case = itrow\n",
    "    sims, neigh_idx = cbr.cb.get_neighbours(X_test[i][np.newaxis, :], k=1, get_solutions=False)\n",
    "    neigh_idx = neigh_idx[0, 0]\n",
    "    similar_case = df_cb.iloc[neigh_idx]\n",
    "\n",
    "    # We only want the text and mortality from the similar case\n",
    "    sc_text = similar_case['text']\n",
    "    sc_dies = similar_case['DIES']\n",
    "\n",
    "    # Apply prepro to remove clutter\n",
    "    # <Nothing to do>\n",
    "\n",
    "    # Truncate middle if resulting text is longer than max_chars\n",
    "    if len(sc_text) > max_chars:\n",
    "        print(f'(i) Text exceeds the max char limit ({len(sc_text)}) in entry {similar_case.index}. Middle-truncating to {max_chars}...')\n",
    "        sc_text = sc_text[:(max_chars//2)] + sc_text[-(max_chars//2):]\n",
    "        print(f'... Result truncate: {len(sc_text)}')\n",
    "\n",
    "    # Wrap into appropriate format\n",
    "    formatted_input = json.dumps({'REPORT': sc_text})\n",
    "\n",
    "    # Prepare query\n",
    "    data = {'model': model,  # Explicit model to use\n",
    "        'options': {\n",
    "            'num_ctx': n_ctx * 1024,\n",
    "            'temperature': temp,\n",
    "            'seed': SEED,\n",
    "            'top_k': top_k,\n",
    "            'top_p': top_p\n",
    "            },\n",
    "        'keep-alive': -1,  # Keep connection open\n",
    "        'prompt': formatted_input,\n",
    "        'stream': False,  # Wait and return all the result at once\n",
    "        'format': {    \n",
    "        'type': 'object',\n",
    "        'properties': {\n",
    "            'SUMMARY': {\n",
    "                'type': 'string'\n",
    "            }\n",
    "        },\n",
    "        'required': [\n",
    "            'SUMMARY'\n",
    "        ]\n",
    "        }\n",
    "    }\n",
    "    # Prepares query\n",
    "    data = json.dumps(data)\n",
    "    cookies = {\n",
    "        '_oauth2_proxy': auth_cookie}\n",
    "    headers = {\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "    }\n",
    "\n",
    "    response = requests.post(instance, cookies=cookies, headers=headers, data=data)\n",
    "    json_response = json.loads(response.text)['response']\n",
    "    dict_response = json.loads(json_response)\n",
    "    dict_response['DIES'] = 'YES' if sc_dies else 'NO' # Mortality outcome of neighbour\n",
    "    responses[hadm_idx] = dict_response # Keeps the dictionary version of the json response\n",
    "    ii+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summarizations\n",
    "df_responses = pd.DataFrame(responses).T\n",
    "summary_id = f'neighbour_summary_{max_chars}mc_ss{subsamp_size}'\n",
    "df_responses.to_csv(f'{mimicpath}/summaries/{summary_id}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
