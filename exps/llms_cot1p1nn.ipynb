{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoT mortality prediction using precomputed summaries from both, test entries and examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Project root path\n",
    "pjpath = ''\n",
    "\n",
    "# Hacky way of finding the project's root path. Do not rely on this, set your own pjpath!\n",
    "for p in Path.cwd().parents:\n",
    "    if p.stem == 'llms4mortality':\n",
    "        pjpath = p\n",
    "        break\n",
    "\n",
    "print(f'> Project path is {pjpath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant paths\n",
    "mimicpath = pjpath / 'data/mimiciv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Controls which data to load\n",
    "samp_size = 5000\n",
    "balanced_data = True\n",
    "\n",
    "#base_model = 'iv_ll3_direct'  # Needs to be available\n",
    "\n",
    "base_models = [\n",
    "    'll3',\n",
    "]\n",
    "\n",
    "# Slice long input: Just keep up to max_words of each text\n",
    "max_chars = 22000\n",
    "subsamp_size = 200  # Number of entries to test model with\n",
    "\n",
    "# LLM parameters\n",
    "input_type = 'S'    # (R)eport, (R)eport and (C)hart data as json\n",
    "output_type = {     # Maps type of output with the actual format that will be requested in the query\n",
    "    'M': {  # Just mortality      \n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'DIES': {\n",
    "                    'enum': ['YES', 'NO']\n",
    "                }\n",
    "            },\n",
    "            'required': [\n",
    "                'DIES'\n",
    "            ]\n",
    "            }\n",
    "    }\n",
    "\n",
    "# This is the collection of columns that contains the relevant patient info that will be provided to the LLM with the text report\n",
    "# Remapping some column names to make them more significant in the prompt\n",
    "pdc_remap = {\n",
    "    'age': 'AGE',\n",
    "    'gender': 'GENDER',\n",
    "    'marital_status': 'MARITAL STATUS',\n",
    "    'race': 'RACE',\n",
    "    'diagnose_group_description': 'BROAD DIAGNOSIS',\n",
    "    'diagnose_group_mortality': 'MORTALITY RISK',\n",
    "    'insurance': 'INSURANCE',\n",
    "    #'text': 'REPORT'\n",
    "}\n",
    "\n",
    "n_ctx = 32   # Context length (x 1024)\n",
    "temp = 0.0 # Temperature option for the LLM. The greater, the more creative the answer (def 0.1)\n",
    "top_k = 20\n",
    "top_p = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load precomputed summaries (both from test entries and neighbours)\n",
    "\n",
    "summary_id = f'summary_{max_chars}mc_ss{subsamp_size}'\n",
    "neigh_summary_id = f'neighbour_summary_{max_chars}mc_ss{subsamp_size}_1p1'\n",
    "df_test_summaries = pd.read_csv(f'{mimicpath}/summaries/{summary_id}.csv', index_col=0)\n",
    "df_neigh_summaries_lives = pd.read_csv(f'{mimicpath}/summaries/{neigh_summary_id}_lives.csv', index_col=0)\n",
    "df_neigh_summaries_dies = pd.read_csv(f'{mimicpath}/summaries/{neigh_summary_id}_dies.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fire instance\n",
    "# NOTE: Assuming fixed input type (S) and output (M)\n",
    "i_type_id = input_type\n",
    "o_type_id = 'M'\n",
    "o_type_format = output_type[o_type_id]\n",
    "\n",
    "instance = 'http://localhost:11434/api/generate'\n",
    "auth_cookie = ''\n",
    "\n",
    "for base_model in base_models:\n",
    "    responses = {}\n",
    "    i=1\n",
    "\n",
    "    for index, row in df_test_summaries.iterrows():\n",
    "        print(f'>> Processing row {i} out of {len(df_test_summaries)}', end='\\r')\n",
    "\n",
    "        # Get text from entry\n",
    "        text = row['SUMMARY']\n",
    "\n",
    "        # Truncate middle if resulting text is longer than max_chars\n",
    "        if len(text) > max_chars:\n",
    "            print(f'(i) Text exceeds the max char limit ({len(text)}) in entry {index}. Middle-truncating to {max_chars}...')\n",
    "            text = text[:(max_chars//2)] + text[-(max_chars//2):]\n",
    "            print(f'... Result truncate: {len(text)}')\n",
    "        \n",
    "        formatted_input = json.dumps({'REPORT': text})\n",
    "        \n",
    "        # Get neighour test from entry\n",
    "        neigh_lives_text = df_neigh_summaries_lives.loc[index]['SUMMARY']\n",
    "        neigh_lives_outcome = df_neigh_summaries_lives.loc[index]['DIES']\n",
    "        # Truncate middle if resulting text is longer than max_chars\n",
    "        if len(neigh_lives_text) > max_chars:\n",
    "            print(f'(i) Neighbour text exceeds the max char limit ({len(neigh_lives_text)}) in entry {index}. Middle-truncating to {max_chars}...')\n",
    "            neigh_lives_text = neigh_lives_text[:(max_chars//2)] + neigh_lives_text[-(max_chars//2):]\n",
    "            print(f'... Result truncate: {len(neigh_lives_text)}')\n",
    "\n",
    "        formatted_input_neigh_lives = json.dumps({'REPORT': neigh_lives_text})\n",
    "        formatted_output_neigh_lives = json.dumps({'DIES': neigh_lives_outcome})\n",
    "\n",
    "        # Get neighour test from entry\n",
    "        neigh_dies_text = df_neigh_summaries_dies.loc[index]['SUMMARY']\n",
    "        neigh_dies_outcome = df_neigh_summaries_dies.loc[index]['DIES']\n",
    "        # Truncate middle if resulting text is longer than max_chars\n",
    "        if len(neigh_dies_text) > max_chars:\n",
    "            print(f'(i) Neighbour text exceeds the max char limit ({len(neigh_dies_text)}) in entry {index}. Middle-truncating to {max_chars}...')\n",
    "            neigh_dies_text = neigh_dies_text[:(max_chars//2)] + neigh_dies_text[-(max_chars//2):]\n",
    "            print(f'... Result truncate: {len(neigh_dies_text)}')\n",
    "\n",
    "        formatted_input_neigh_dies = json.dumps({'REPORT': neigh_dies_text})\n",
    "        formatted_output_neigh_dies = json.dumps({'DIES': neigh_dies_outcome})\n",
    "\n",
    "        # Builds full CoT prompt with example\n",
    "        cot1p1_prompt = \"\\nThe following is an example of the medical text report of a patient that died within 30 days of hospital discharge:\\n\"\n",
    "        cot1p1_prompt += formatted_input_neigh_dies\n",
    "        cot1p1_prompt += \"\\nIn contrast, the following is an example of the medical text report of a patient that stayed alive after 30 days of hospital discharge:\\n\"\n",
    "        cot1p1_prompt += formatted_input_neigh_lives\n",
    "        cot1p1_prompt += \"\\nNow decide if the following text report corresponds to a patient who is likely to die within 30 days of medical discharge as per your main instructions, and considering that the previous examples corresponds to two separate medical cases that are clinically similar to the one you have to resolve now:\\n\"\n",
    "        cot1p1_prompt += formatted_input\n",
    "\n",
    "        model = f'{base_model}_{i_type_id}_{o_type_id}'\n",
    "        data = {'model': model,  # Explicit model to use\n",
    "            'options': {\n",
    "                'num_ctx': n_ctx * 1024,\n",
    "                'temperature': temp,\n",
    "                'seed': SEED,\n",
    "                'top_k': top_k,\n",
    "                'top_p': top_p\n",
    "                },\n",
    "            'keep-alive': -1,  # Keep connection open\n",
    "            'prompt': cot1p1_prompt,\n",
    "            'stream': False,  # Wait and return all the result at once\n",
    "            'format': o_type_format\n",
    "        }\n",
    "\n",
    "        # Prepares query\n",
    "        data = json.dumps(data)\n",
    "        cookies = {\n",
    "            '_oauth2_proxy': auth_cookie}\n",
    "        headers = {\n",
    "            'Content-Type': 'application/x-www-form-urlencoded',\n",
    "        }\n",
    "        cot1nn_response = requests.post(instance, cookies=cookies, headers=headers, data=data)\n",
    "        cot1nn_response_json = json.loads(cot1nn_response.text)['response']\n",
    "        responses[index] = json.loads(cot1nn_response_json) # Keeps the dictionary version of the json response\n",
    "        #summarized_texts.append(json.loads(response)['summary'])\n",
    "\n",
    "        i+=1\n",
    "\n",
    "    # Export results\n",
    "    df_responses = pd.DataFrame(responses).T\n",
    "    test_id = f'{base_model}_{i_type_id}_{o_type_id}_{n_ctx}k_t{str(temp).replace('.', '')}'\n",
    "    df_responses.to_csv(f'../results/ex2/{test_id}.csv')\n",
    "    print(f'\\n')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
