{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoT mortality prediction using precomputed summaries from both, test entries and examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Project path is /home/daucco/ownCloud-UPM/CBR/medical-cbr\n"
     ]
    }
   ],
   "source": [
    "## Project root path\n",
    "pjpath = ''\n",
    "\n",
    "# Hacky way of finding the project's root path. Do not rely on this, set your own pjpath!\n",
    "for p in Path.cwd().parents:\n",
    "    if p.stem == 'llms4mortality':\n",
    "        pjpath = p\n",
    "        break\n",
    "\n",
    "print(f'> Project path is {pjpath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant paths\n",
    "mimicpath = pjpath / 'data/mimiciv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Controls which data to load\n",
    "samp_size = 5000\n",
    "balanced_data = True\n",
    "\n",
    "#base_model = 'iv_ll3_direct'  # Needs to be available\n",
    "\n",
    "base_models = [\n",
    "    'll3',\n",
    "]\n",
    "\n",
    "# Slice long input: Just keep up to max_words of each text\n",
    "max_chars = 22000\n",
    "subsamp_size = 200  # Number of entries to test model with\n",
    "\n",
    "# LLM parameters\n",
    "input_type = 'S'    # (R)eport, (R)eport and (C)hart data as json\n",
    "output_type = {     # Maps type of output with the actual format that will be requested in the query\n",
    "    'M': {  # Just mortality      \n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'DIES': {\n",
    "                    'enum': ['YES', 'NO']\n",
    "                }\n",
    "            },\n",
    "            'required': [\n",
    "                'DIES'\n",
    "            ]\n",
    "            }\n",
    "    }\n",
    "\n",
    "# This is the collection of columns that contains the relevant patient info that will be provided to the LLM with the text report\n",
    "# Remapping some column names to make them more significant in the prompt\n",
    "pdc_remap = {\n",
    "    'age': 'AGE',\n",
    "    'gender': 'GENDER',\n",
    "    'marital_status': 'MARITAL STATUS',\n",
    "    'race': 'RACE',\n",
    "    'diagnose_group_description': 'BROAD DIAGNOSIS',\n",
    "    'diagnose_group_mortality': 'MORTALITY RISK',\n",
    "    'insurance': 'INSURANCE',\n",
    "    #'text': 'REPORT'\n",
    "}\n",
    "\n",
    "n_ctx = 32   # Context length (x 1024)\n",
    "temp = 0.0 # Temperature option for the LLM. The greater, the more creative the answer (def 0.1)\n",
    "top_k = 20\n",
    "top_p = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load precomputed summaries (both from test entries and neighbours)\n",
    "\n",
    "summary_id = f'summary_{max_chars}mc_ss{subsamp_size}'\n",
    "neigh_summary_id = f'neighbour_summary_{max_chars}mc_ss{subsamp_size}'\n",
    "df_test_summaries = pd.read_csv(f'{mimicpath}/summaries/{summary_id}.csv', index_col=0)\n",
    "df_neigh_summaries = pd.read_csv(f'{mimicpath}/summaries/{neigh_summary_id}.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [BASE MODEL]: iv_ll3_direct\n",
      ">> Processing row 100 out of 100\n",
      "\n",
      "> [BASE MODEL]: medgenius32b\n",
      ">> Processing row 100 out of 100\n",
      "\n",
      "> [BASE MODEL]: dsmedical8b\n",
      ">> Processing row 100 out of 100\n",
      "\n",
      "> [BASE MODEL]: biomistral7b\n",
      ">> Processing row 100 out of 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fire instance\n",
    "# NOTE: Assuming fixed input type (S) and output (M)\n",
    "i_type_id = input_type\n",
    "o_type_id = 'M'\n",
    "o_type_format = output_type[o_type_id]\n",
    "\n",
    "instance = 'http://localhost:11434/api/generate'\n",
    "auth_cookie = ''\n",
    "\n",
    "for base_model in base_models:\n",
    "    print(f'> [BASE MODEL]: {base_model}')\n",
    "    responses = {}\n",
    "    i=1\n",
    "\n",
    "    for index, row in df_test_summaries.iterrows():\n",
    "        print(f'>> Processing row {i} out of {len(df_test_summaries)}', end='\\r')\n",
    "\n",
    "        # Get text from entry\n",
    "        text = row['SUMMARY']\n",
    "\n",
    "        # Truncate middle if resulting text is longer than max_chars\n",
    "        if len(text) > max_chars:\n",
    "            print(f'(i) Text exceeds the max char limit ({len(text)}) in entry {index}. Middle-truncating to {max_chars}...')\n",
    "            text = text[:(max_chars//2)] + text[-(max_chars//2):]\n",
    "            print(f'... Result truncate: {len(text)}')\n",
    "\n",
    "        # Get neighour test from entry\n",
    "        neigh_text = df_neigh_summaries.loc[index]['SUMMARY']\n",
    "        neigh_dies = df_neigh_summaries.loc[index]['DIES']\n",
    "\n",
    "        # Truncate middle if resulting text is longer than max_chars\n",
    "        if len(neigh_text) > max_chars:\n",
    "            print(f'(i) Neighbour text exceeds the max char limit ({len(neigh_text)}) in entry {index}. Middle-truncating to {max_chars}...')\n",
    "            neigh_text = neigh_text[:(max_chars//2)] + neigh_text[-(max_chars//2):]\n",
    "            print(f'... Result truncate: {len(neigh_text)}')\n",
    "\n",
    "        formatted_input = json.dumps({'REPORT': text})\n",
    "        formatted_input_neigh = json.dumps({'REPORT': neigh_text})\n",
    "        formatted_output_neigh = json.dumps({'DIES': neigh_dies})\n",
    "\n",
    "        # Builds full CoT prompt with example\n",
    "        #cot1nn_prompt = \"Here's an example of the INPUT and OUTPUT of the mortality prediction task using a medical that was previously deemed similar to the one to resolve:\"\n",
    "        \n",
    "        cot1nn_prompt = \"\\nBelow is an example of the medical text report from which you will have to decide if the patient will die within 30 days of their medical discharge:\\n\"\n",
    "        cot1nn_prompt += formatted_input_neigh\n",
    "        cot1nn_prompt += \"\\nAnd here's its solution:\\n\"\n",
    "        cot1nn_prompt += formatted_output_neigh\n",
    "        cot1nn_prompt += \"\\nNow decide if the following text report corresponds to a patient who is likely to die within 30 days of medical discharge as per your main instructions, and considering that the previous example corresponds to a medical case that is similar to the one you have to resolve now:\\n\"\n",
    "        cot1nn_prompt += formatted_input\n",
    "\n",
    "        model = f'{base_model}_{i_type_id}_{o_type_id}'\n",
    "        data = {'model': model,  # Explicit model to use\n",
    "            'options': {\n",
    "                'num_ctx': n_ctx * 1024,\n",
    "                'temperature': temp,\n",
    "                'seed': SEED,\n",
    "                'top_k': top_k,\n",
    "                'top_p': top_p\n",
    "                },\n",
    "            'keep-alive': -1,  # Keep connection open\n",
    "            'prompt': cot1nn_prompt,\n",
    "            'stream': False,  # Wait and return all the result at once\n",
    "            'format': o_type_format\n",
    "        }\n",
    "\n",
    "        # Prepares query\n",
    "        data = json.dumps(data)\n",
    "        cookies = {\n",
    "            '_oauth2_proxy': auth_cookie}\n",
    "        headers = {\n",
    "            'Content-Type': 'application/x-www-form-urlencoded',\n",
    "        }\n",
    "        cot1nn_response = requests.post(instance, cookies=cookies, headers=headers, data=data)\n",
    "        cot1nn_response_json = json.loads(cot1nn_response.text)['response']\n",
    "        responses[index] = json.loads(cot1nn_response_json) # Keeps the dictionary version of the json response\n",
    "        #summarized_texts.append(json.loads(response)['summary'])\n",
    "\n",
    "        i+=1\n",
    "\n",
    "    # Export results\n",
    "    df_responses = pd.DataFrame(responses).T\n",
    "    test_id = f'{base_model}_{i_type_id}_{o_type_id}_{n_ctx}k_t{str(temp).replace('.', '')}'\n",
    "    df_responses.to_csv(f'../results/ex2/{test_id}.csv')\n",
    "    print(f'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
