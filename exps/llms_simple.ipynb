{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Project path is /home/daucco/ownCloud-UPM/CBR/medical-cbr\n"
     ]
    }
   ],
   "source": [
    "## Project root path\n",
    "pjpath = ''\n",
    "\n",
    "# Hacky way of finding the project's root path. Do not rely on this, set your own pjpath!\n",
    "for p in Path.cwd().parents:\n",
    "    if p.stem == 'llms4mortality':\n",
    "        pjpath = p\n",
    "        break\n",
    "\n",
    "print(f'> Project path is {pjpath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant paths\n",
    "mimicpath = pjpath / 'data/mimiciv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Controls which data to load\n",
    "samp_size = 20000\n",
    "balanced_data = True\n",
    "\n",
    "base_models = [\n",
    "    'iv_ll3_direct',\n",
    "    'medgenius32b',\n",
    "    'dsmedical8b',\n",
    "    'biomistral7b'\n",
    "]\n",
    "\n",
    "# Controls which llm to fire\n",
    "input_type = ['R', 'RC']    # (R)eport, (R)eport and (C)hart data as json, (S)ummary (this needs to be precomputed!)\n",
    "output_type = {     # Maps type of output with the actual format that will be requested in the query\n",
    "    'M': {  # Just mortality      \n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'DIES': {\n",
    "                    'enum': ['YES', 'NO']\n",
    "                }\n",
    "            },\n",
    "            'required': [\n",
    "                'DIES'\n",
    "            ]\n",
    "           },\n",
    "    'PM': {  # Prognosis and mortality      \n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'PROGNOSIS': {\n",
    "                    'type': 'string'\n",
    "                },\n",
    "                'DIES': {\n",
    "                    'enum': ['YES', 'NO']\n",
    "                }\n",
    "            },\n",
    "            'required': [\n",
    "                'PROGNOSIS',\n",
    "                'DIES'\n",
    "            ]\n",
    "            }\n",
    "    }\n",
    "\n",
    "# This is the collection of columns that contains the relevant patient info that will be provided to the LLM with the text report\n",
    "# Remapping some column names to make them more significant in the prompt\n",
    "pdc_remap = {\n",
    "    'age': 'AGE',\n",
    "    'gender': 'GENDER',\n",
    "    'marital_status': 'MARITAL STATUS',\n",
    "    'race': 'RACE',\n",
    "    'diagnose_group_description': 'BROAD DIAGNOSIS',\n",
    "    'diagnose_group_mortality': 'MORTALITY RISK',\n",
    "    'insurance': 'INSURANCE',\n",
    "    #'text': 'REPORT'\n",
    "}\n",
    "\n",
    "num_ctx = [16]   # Context length (x 1024)\n",
    "mortality_span = ['30days'] # 30days, amonth How to ask the LLM for mortality (count days vs month)\n",
    "temps = [0.1] #[0.0, *0.1, 0.3, 0.7, 1.0]   # Temperature option for the LLM. The greater, the more creative the answer (def 0.1)\n",
    "\n",
    "\n",
    "# Fixed model params\n",
    "top_k = 20  # *20\n",
    "top_p = 0.5 # *.5\n",
    "\n",
    "# Slice long input: Just keep up to max_words of each text\n",
    "max_chars = 22000\n",
    "subsamp_size = 100  # 100 Number of entries to test model with. This number will be used to load summaries from disks\n",
    "\n",
    "# NOTE use this only to limit the number of entries to process in the experiment\n",
    "#   Smaller than subsamo_size\n",
    "minisample = -1\n",
    "\n",
    "# If set, attempts to load note summaries from disk and use those instead of the raw reports.\n",
    "summaries_dir = f'{mimicpath}/summaries/'\n",
    "\n",
    "prepend_columns = ['age', 'gender', 'insurance', 'marital_status', 'race']   # Columns in base dataframe to prepend to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load precomputed dataframe. Keeps only hadm_id and delta_days_dod (to find patients that died after n days discharge)\n",
    "# Transform to boolean (patient died within 30 days after discharge)\n",
    "df = pd.read_csv(mimicpath / f'mimiciv_4_mortality_S{samp_size}{'_balanced' if balanced_data else ''}.csv.gz')\n",
    "df['delta_days_dod'] = df['delta_days_dod'].apply(lambda x: x > 0 and x <= 30)\n",
    "\n",
    "# Load precomputed splits\n",
    "with open(mimicpath / f'hadmid_splits_S{samp_size}{'_balanced' if balanced_data else ''}.json', 'r') as ifile:\n",
    "    splits_hadmids = json.load(ifile)\n",
    "\n",
    "# Load sorted hadm_ids from disk\n",
    "with open(mimicpath / f'hadmid_sorted_S{samp_size}{'_balanced' if balanced_data else ''}.json', 'r') as ifile:\n",
    "    emb_hadmids = json.load(ifile)['HADM_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads test split\n",
    "df_test = df.set_index('hadm_id').loc[splits_hadmids['test']]\n",
    "\n",
    "# Gets subsample\n",
    "if subsamp_size:\n",
    "    print(f'> Subsampling test data to {subsamp_size}...')\n",
    "    df_test = df_test.sample(subsamp_size, random_state=SEED)\n",
    "    \n",
    "# Middle-truncates long texts...\n",
    "df_test['text'] = df_test['text'].apply(lambda x: x[:(max_chars//2)] + x[-(max_chars//2):] if len(x) > max_chars else x)\n",
    "\n",
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if minisample > 0:\n",
    "    df_test = df_test.iloc[:minisample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ollama instance and run in batch\n",
    "\n",
    "tests = {}  # Will keep raw responses\n",
    "instance = 'http://localhost:11434/api/generate'\n",
    "auth_cookie = ''\n",
    "\n",
    "for base_model in base_models:\n",
    "    print(f'> [BASE MODEL]: {base_model}')\n",
    "\n",
    "    for i_type in input_type:\n",
    "\n",
    "        i_type_id = i_type\n",
    "\n",
    "        if i_type == 'R':\n",
    "            # Raw report notes as input\n",
    "            # Preprends patient data to text. Replaces underscore with spaces in both, feature name and value\n",
    "            df_test_i = df_test.copy()\n",
    "            #df_test_i['input_query'] = df_test_i.apply(lambda x: ''.join([f'{p_cremap}: {str(x[p_cname]).replace('_', ' ')}\\n' for p_cname, p_cremap in pdc_remap.items()]) + x['text'], axis=1)\n",
    "            df_test_i['input_query'] = df_test_i.apply(lambda x: json.dumps({'REPORT': ''.join([f'{p_cremap}: {str(x[p_cname]).replace('_', ' ')}\\n' for p_cname, p_cremap in pdc_remap.items()]) + x['text']}), axis=1)\n",
    "\n",
    "        elif i_type == 'S':\n",
    "            # NOTE: Assumes summary has already been loaded in text column\n",
    "            \n",
    "            ## Load summary\n",
    "            summaries_path = f'{mimicpath}/summaries/summary_{max_chars}mc_ss{subsamp_size}.csv'\n",
    "\n",
    "            df_summaries = pd.read_csv(summaries_path, index_col=0)\n",
    "\n",
    "            missing_summaries = set(df_test.index) - set(df_test.index)\n",
    "            if len(missing_summaries) > 0:\n",
    "                print(f'(!) The following hadm_ids have no summary: {missing_summaries}')\n",
    "                print(f'\\tOriginal text will be used insted...')\n",
    "\n",
    "            # Updates dataframe with summaries (only with the ones available)\n",
    "            df_test = pd.merge(df_test, df_summaries, left_index=True, right_index=True, how='left')\n",
    "            df_test['SUMMARY'] = df_test.apply(lambda x: x['text'] if x['SUMMARY'] != x['SUMMARY'] else x['SUMMARY'], axis=1)\n",
    "\n",
    "            # Middle-truncates long texts...\n",
    "            df_test['SUMMARY'] = df_test['SUMMARY'].apply(lambda x: x[:(max_chars//2)] + x[-(max_chars//2):] if len(x) > max_chars else x)\n",
    "            ## End load summary\n",
    "\n",
    "\n",
    "\n",
    "            df_test_i = df_test.copy()\n",
    "            df_test_i['input_query'] = df_test_i['SUMMARY'].apply(lambda x: json.dumps({'REPORT': x}))\n",
    "\n",
    "        elif i_type == 'RC':\n",
    "            # Input as json with relevant patient data in separate fields\n",
    "            df_test_i = df_test.copy()\n",
    "            df_test_i['input_query'] = df_test_i.apply(lambda x: json.dumps({p_cremap: str(x[p_cname]).replace('_', ' ') for p_cname, p_cremap in pdc_remap.items()} | {'REPORT': x['text']}), axis=1)\n",
    "\n",
    "        else:\n",
    "            print(i_type)\n",
    "            raise NotImplementedError\n",
    "\n",
    "        print(f'> [INPUT_TYPE]: {i_type_id}')\n",
    "\n",
    "        for o_type_id, o_type_format in output_type.items():\n",
    "            for n_ctx, m_span, temp in product(num_ctx, mortality_span, temps):\n",
    "\n",
    "                # NOTE: Input type R and S use the same base model:\n",
    "                i_type_id_model = i_type_id\n",
    "\n",
    "                # Resolves model\n",
    "                # # Models need to exists in the online instance!\n",
    "                model = f'{base_model}_{i_type_id_model}_{o_type_id}_{m_span}'\n",
    "\n",
    "                print(f'> [OUTPUT_TYPE]: {o_type_id}, ({n_ctx} ctx, {m_span} span, {temp} temp)')\n",
    "\n",
    "                responses = {}\n",
    "                i=1\n",
    "                for index, row in df_test_i.iterrows():\n",
    "                    # Iterates every query in the test set\n",
    "                    print(f'> Processing input {i} out of {len(df_test_i)}...', end='\\r')\n",
    "\n",
    "                    #patient_input = json.dumps({row.name: row['input_query'][:20000]})\n",
    "                    fromatted_input = row['input_query']\n",
    "\n",
    "                    data = {'model': model,  # Explicit model to use\n",
    "                            #'options': {'num_ctx': n_ctx * 1024, 'temperature': temp},\n",
    "                            'options': {\n",
    "                                'num_ctx': n_ctx * 1024,\n",
    "                                'temperature': temp, # 0?\n",
    "                                'seed': SEED,\n",
    "                                'top_k': top_k,\n",
    "                                'top_p': top_p\n",
    "                                },\n",
    "                            #'options': {'seed': 55},\n",
    "                            'keep-alive': -1,  # Keep connection open\n",
    "                            'prompt': fromatted_input,\n",
    "                            'stream': False,  # Wait and return all the result at once\n",
    "                            'format': o_type_format,\n",
    "                            #'seed': SEED\n",
    "                        }\n",
    "                    # Prepares query\n",
    "                    data = json.dumps(data)\n",
    "                    cookies = {\n",
    "                        '_oauth2_proxy': auth_cookie}\n",
    "                    headers = {\n",
    "                        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "                    }\n",
    "\n",
    "                    response = requests.post(instance, cookies=cookies, headers=headers, data=data)\n",
    "                    json_response = json.loads(response.text)['response']\n",
    "                    responses[index] = json.loads(json_response) # Keeps the dictionary version of the json response\n",
    "                    #summarized_texts.append(json.loads(response)['summary'])\n",
    "                    i+=1\n",
    "\n",
    "                # Export results\n",
    "                df_responses = pd.DataFrame(responses).T\n",
    "                test_id = f'{base_model}_{i_type_id}_{o_type_id}_{m_span}_{n_ctx}k_t{str(temp).replace('.', '')}'\n",
    "                df_responses.to_csv(f'results/ex1/{test_id}.csv')\n",
    "                tests[test_id] = df_responses\n",
    "                print(f'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
