{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model with LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support as prf\n",
    "from sklearn.metrics import matthews_corrcoef as mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "\n",
    "SEED = 42\n",
    "samp_size = 5000   # Use the same as the precomputed df to test on\n",
    "\n",
    "# List of embedding to load from disk (name of embedding model used)\n",
    "modnames = [\n",
    "    #'all-distilroberta-v1',\n",
    "    #'medicalai/ClinicalBERT',\n",
    "    #'emilyalsentzer/Bio_Discharge_Summary_BERT',\n",
    "    'nazyrova/clinicalBERT'\n",
    "    ]\n",
    "\n",
    "# all-distilroberta-v1                          ## Non-specific\n",
    "# medicalai/ClinicalBERT                        ## Healthcare-specific\n",
    "# emilyalsentzer/Bio_Discharge_Summary_BERT     ## MIMIC-III discharge notes\n",
    "# nazyrova/clinicalBERT                         ## MIIMC-IV discharge notes\n",
    "\n",
    "truncation_side = 'right' # right middle \n",
    "balanced_data = True\n",
    "summaries = False\n",
    "withprepended = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR fixed hyperparams\n",
    "lr_base_kwargs = {\n",
    "    'penalty': 'elasticnet',\n",
    "    'max_iter': 1000,\n",
    "    'l1_ratio': .5,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': SEED\n",
    "}\n",
    "\n",
    "# LR adjustable and testable params\n",
    "lr_tol = [.0001]\n",
    "lr_C = [20.0, 50.0]\n",
    "lr_solver = [\n",
    "    #'liblinear',\n",
    "    #'lbfgs',\n",
    "    'saga'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Project path is /home/daucco/ownCloud-UPM/CBR/llms4mortality\n"
     ]
    }
   ],
   "source": [
    "## Project root path\n",
    "pjpath = ''\n",
    "\n",
    "# Hacky way of finding the project's root path. Do not rely on this, set your own pjpath!\n",
    "for p in Path.cwd().parents:\n",
    "    if p.stem == 'llms4mortality':\n",
    "        pjpath = p\n",
    "        break\n",
    "\n",
    "print(f'> Project path is {pjpath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to your MIMIC-IV path where discharge, patients and admissions tables are located\n",
    "mimicpath = pjpath / 'data/mimiciv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load precomputed dataframe. Keeps only hadm_id and delta_days_dod (to find patients that died after n days discharge)\n",
    "# Transform to boolean (patient died within 30 days after discharge)\n",
    "df = pd.read_csv(mimicpath / f'mimiciv_4_mortality_S{samp_size}{'_balanced' if balanced_data else ''}.csv.gz')[['hadm_id', 'delta_days_dod']]\n",
    "df['delta_days_dod'] = df['delta_days_dod'].apply(lambda x: x > 0 and x <= 30)  # Only keeps this columns, so there's no need to rename it for eval\n",
    "\n",
    "# Load precomputed splits\n",
    "with open(mimicpath / f'hadmid_splits_S{samp_size}{'_balanced' if balanced_data else ''}.json', 'r') as ifile:\n",
    "    splits_hadmids = json.load(ifile)\n",
    "\n",
    "# Load sorted hadm_ids from disk\n",
    "with open(mimicpath / f'hadmid_sorted_S{samp_size}{'_balanced' if balanced_data else ''}.json', 'r') as ifile:\n",
    "    emb_hadmids = json.load(ifile)['HADM_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading embeddings from embeddings_nazyrovaclinicalBERT_S5000_Tright_balanced_PR.npy...\n",
      ">> EX IDX: 1/2\n",
      "> [EX. CONFIG]: model:nazyrovaclinicalBERT\n",
      "tol:0.0001, C:20.0, solver:saga\n",
      "> Fitting LR model on samples of shape: (1502, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daucco/ownCloud-UPM/CBR/llms4mortality/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/daucco/ownCloud-UPM/CBR/llms4mortality/.venv/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/daucco/ownCloud-UPM/CBR/llms4mortality/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> EX IDX: 2/2\n",
      "> [EX. CONFIG]: model:nazyrovaclinicalBERT\n",
      "tol:0.0001, C:50.0, solver:saga\n",
      "> Fitting LR model on samples of shape: (1502, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daucco/ownCloud-UPM/CBR/llms4mortality/.venv/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Fits and evaluates models for each type of embeddings\n",
    "\n",
    "res = pd.DataFrame(columns=['features', 'tol', 'C', 'solver', 'f1_micro', 'f1_macro', 'mcc'])\n",
    "\n",
    "e_n = len(modnames) * len(lr_tol) * len(lr_C) * len(lr_solver)\n",
    "e_count = 1\n",
    "for  i, modname in enumerate(modnames):\n",
    "    # Load embedding model (need to be pregenerated in disk)\n",
    "    modname = re.sub('[^a-zA-Z0-9]+', '', modname)\n",
    "    mod_fname = f'embeddings_{modname}_{'summary_' if summaries else ''}S{samp_size}_T{truncation_side}{'_balanced' if balanced_data else ''}{'_PR' if withprepended else ''}.npy'\n",
    "    \n",
    "    print(f'> Loading embeddings from {mod_fname}...')\n",
    "    embeddings = np.load(mimicpath / mod_fname)\n",
    "\n",
    "    # Locates train and test data\n",
    "    hadm2idx = {hadm: i for i, hadm in enumerate(emb_hadmids)}\n",
    "    X_train = embeddings[[hadm2idx[hadm] for hadm in splits_hadmids['train']]]\n",
    "    y_train = df.set_index('hadm_id').loc[splits_hadmids['train']]\n",
    "    X_test = embeddings[[hadm2idx[hadm] for hadm in splits_hadmids['test']]]\n",
    "    y_test = df.set_index('hadm_id').loc[splits_hadmids['test']]\n",
    "\n",
    "    for tol in lr_tol:\n",
    "        for C in lr_C:\n",
    "            for solver in lr_solver:\n",
    "                # Fits LR\n",
    "                print(f'>> EX IDX: {e_count}/{e_n}')\n",
    "                print(f'> [EX. CONFIG]: model:{modname}\\ntol:{tol}, C:{C}, solver:{solver}')\n",
    "                print(f'> Fitting LR model on samples of shape: {X_train.shape}')\n",
    "                lr_cla = LogisticRegression(tol=tol, C=C, solver=solver, **lr_base_kwargs).fit(X_train, y_train)\n",
    "\n",
    "                # Eval LR (prf micro macro, mcc)\n",
    "                y_pred = lr_cla.predict(X_test)\n",
    "                \n",
    "                # Save results to results df\n",
    "                res.loc[len(res)] = [modname,\n",
    "                    tol,\n",
    "                    C,\n",
    "                    solver,\n",
    "                    prf(y_test, y_pred, average='micro')[2],\n",
    "                    prf(y_test, y_pred, average='macro')[2],\n",
    "                    mcc(y_test, y_pred)]\n",
    "\n",
    "                e_count += 1\n",
    "\n",
    "# Export results df to disk\n",
    "res.to_csv(f'{pjpath}/exps/results/lr_embeddings_S{samp_size}{'_balanced' if balanced_data else ''}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
